{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\Hussein\\AppData\\Local\\Temp\\ipykernel_13552\\1133486942.py:8: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  BASE_IMG_DIR = \"Example Data-20240208T214429Z-001\\Example Data\\exported\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine  Similarity Score -1 to 1\n",
      "1 indicates identical directionality (very similar),\n",
      "0 indicates orthogonality (not similar),\n",
      "and -1 indicates opposite directionality (very dissimilar).\n",
      "Cosine similarity score for user text prompt: 0.2476407289505005\n",
      "Cosine similarity score for smart story description: 0.18213330209255219\n"
     ]
    }
   ],
   "source": [
    "# Load the CLIP model onto the CPU\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n",
    "\n",
    "# Load the NLP model (spaCy)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Setting up a base directory for the images \n",
    "BASE_IMG_DIR = \"Example Data-20240208T214429Z-001\\Example Data\\exported\"\n",
    "image_name = \"IMG_3155.JPG\"\n",
    "# Prepare the image\n",
    "image_path = f\"{BASE_IMG_DIR}/{image_name}\"\n",
    "\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).to('cpu')\n",
    "\n",
    "# Accept user input for text prompt and smart story description\n",
    "user_text_prompt = \"a photo of people standing outside\"\n",
    "smart_story_description = \"Three individuals outdoors, smiling at the camera and pointing at a park bench.\"\n",
    "\n",
    "# Process the texts with spaCy for named entity recognition (NER)\n",
    "user_doc = nlp(user_text_prompt)\n",
    "story_doc = nlp(smart_story_description)\n",
    "\n",
    "# Extract entities, or use the original text if no entities are found\n",
    "user_entities = ' '.join([ent.text for ent in user_doc.ents]) if user_doc.ents else user_text_prompt\n",
    "story_entities = ' '.join([ent.text for ent in story_doc.ents]) if story_doc.ents else smart_story_description\n",
    "\n",
    "# Tokenize the refined texts\n",
    "text_inputs = clip.tokenize([user_entities, story_entities]).to('cpu')\n",
    "\n",
    "# Calculate the features with CLIP\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    # Normalize the features to unit vectors\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarities = torch.matmul(image_features, text_features.T).cpu().numpy().flatten()\n",
    "\n",
    "# Print the cosine similarity scores\n",
    "print(\"Cosine  Similarity Score -1 to 1\")\n",
    "print(\"1 indicates identical directionality (very similar),\")\n",
    "print(\"0 indicates orthogonality (not similar),\")\n",
    "print(\"and -1 indicates opposite directionality (very dissimilar).\"  )\n",
    "print(f\"Cosine similarity score for user text prompt: {cosine_similarities[0]}\")\n",
    "print(f\"Cosine similarity score for smart story description: {cosine_similarities[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 54\u001b[0m\n\u001b[0;32m     47\u001b[0m coco_dataset \u001b[38;5;241m=\u001b[39m LoadCocoDataset(\n\u001b[0;32m     48\u001b[0m     image_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/coco128/images\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     49\u001b[0m     label_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/coco128/labels/train2017\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     50\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Initialize DataLoader\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class LoadCocoDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # List all image files in the directory\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load label\n",
    "        label_path = os.path.join(self.label_dir, self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        # Assuming the labels are space-separated values: class x_center y_center width height\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    labels.append(np.fromstring(line, sep=' '))\n",
    "\n",
    "        labels = np.array(labels)  # Convert to a NumPy array\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Define your transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize your dataset\n",
    "coco_dataset = LoadCocoDataset(\n",
    "    image_dir='datasets/coco128/images',\n",
    "    label_dir='datasets/coco128/labels/train2017',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader = DataLoader(coco_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_loader\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images, labels)\u001b[38;5;66;03m# - images: a batch of images\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# - labels: the corresponding labels for each image\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for images, labels in data_loader:\n",
    "    print(images, labels)# - images: a batch of images\n",
    "    # - labels: the corresponding labels for each image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\Hussein\\AppData\\Local\\Temp\\ipykernel_13552\\3993601786.py:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  df = pd.read_excel('Example Data-20240208T214429Z-001\\Example Data\\ImageLabels.xlsx', usecols=['Image Name', 'Confidence', 'Instance Count', 'Label'])\n",
      "C:\\Users\\Hussein\\AppData\\Local\\Temp\\ipykernel_13552\\3993601786.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Label'] = filtered_df['Instance Count'].astype(str) + ' ' + filtered_df['Label']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Name</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1) @GreyCupFestival - 109th Grey Cup.jpeg</td>\n",
       "      <td>[2 Adult, 1 Female, 7 Person, 1 Woman, 1 Male,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(10) 17887803224903630.jpeg</td>\n",
       "      <td>[1 Horse, 1 Person, 1 Adult, 1 Female, 1 Woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(12) 17985809330117499.jpeg</td>\n",
       "      <td>[1 Person, 1 Helmet, 1 Motorcycle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(13) 18013990822817757.jpeg</td>\n",
       "      <td>[3 Passport]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(14) 17993584322154200.jpeg</td>\n",
       "      <td>[2 Person, 6 Bird, 1 Glove]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Image Name  \\\n",
       "0  (1) @GreyCupFestival - 109th Grey Cup.jpeg   \n",
       "1                 (10) 17887803224903630.jpeg   \n",
       "2                 (12) 17985809330117499.jpeg   \n",
       "3                 (13) 18013990822817757.jpeg   \n",
       "4                 (14) 17993584322154200.jpeg   \n",
       "\n",
       "                                              Labels  \n",
       "0  [2 Adult, 1 Female, 7 Person, 1 Woman, 1 Male,...  \n",
       "1    [1 Horse, 1 Person, 1 Adult, 1 Female, 1 Woman]  \n",
       "2                 [1 Person, 1 Helmet, 1 Motorcycle]  \n",
       "3                                       [3 Passport]  \n",
       "4                        [2 Person, 6 Bird, 1 Glove]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Example Data-20240208T214429Z-001\\Example Data\\ImageLabels.xlsx', usecols=['Image Name', 'Confidence', 'Instance Count', 'Label'])\n",
    "\n",
    "# Append the instance counts >=1 to the label, e.g: '1 Human'\n",
    "\n",
    "# Filter based on the criteria\n",
    "filtered_df = df[(df['Confidence'] >= 80) & (df['Instance Count'] > 0)]\n",
    "\n",
    "# Append instance count to label\n",
    "filtered_df['Label'] = filtered_df['Instance Count'].astype(str) + ' ' + filtered_df['Label']\n",
    "\n",
    "# Group by 'Image Name' and aggregate labels into a list\n",
    "aggregated_df = filtered_df.groupby('Image Name')['Label'].apply(list).reset_index()\n",
    "\n",
    "# Renaming 'Label' column to 'Labels' to reflect the aggregated data\n",
    "aggregated_df.columns = ['Image Name', 'Labels']\n",
    "\n",
    "# Display the aggregated dataframe\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(1) @GreyCupFestival - 109th Grey Cup.jpeg',\n",
       "  ['2 Adult',\n",
       "   '1 Female',\n",
       "   '7 Person',\n",
       "   '1 Woman',\n",
       "   '1 Male',\n",
       "   '1 Man',\n",
       "   '1 Helmet',\n",
       "   '1 Coat',\n",
       "   '1 Shoe']),\n",
       " ('(10) 17887803224903630.jpeg',\n",
       "  ['1 Horse', '1 Person', '1 Adult', '1 Female', '1 Woman']),\n",
       " ('(12) 17985809330117499.jpeg', ['1 Person', '1 Helmet', '1 Motorcycle']),\n",
       " ('(13) 18013990822817757.jpeg', ['3 Passport']),\n",
       " ('(14) 17993584322154200.jpeg', ['2 Person', '6 Bird', '1 Glove'])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict = aggregated_df.set_index('Image Name').to_dict()['Labels']\n",
    "\n",
    "# Show the first few elements of the dictionary to verify\n",
    "list(labels_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 49)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming 'user_text_prompt' and 'smart_story_descriptions' are provided as examples\n",
    "user_text_prompt = \"My new motor bike\"\n",
    "smart_story_descriptions = [\n",
    "    \"Motor Bike club\"\n",
    "]\n",
    "\n",
    "# Combine the text inputs with image labels for tokenization\n",
    "# First, convert the labels dictionary into a single string per image\n",
    "labels_texts = ['; '.join(labels) for labels in labels_dict.values()]\n",
    "\n",
    "# Combine all texts into a single list for vectorization\n",
    "all_texts = [user_text_prompt] + smart_story_descriptions + labels_texts\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Tokenize and vectorize the texts\n",
    "X = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Convert the result to an array to see the tokenized form\n",
    "tokenized_array = X.toarray()\n",
    "\n",
    "# Show the shape of the tokenized array as a simple verification step\n",
    "tokenized_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hussein\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('(14) 17993584322154200.jpeg', 0.96890765)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_texts(texts):\n",
    "    \"\"\"\n",
    "    Encode a list of texts into embeddings using a pre-trained BERT model.\n",
    "    \"\"\"\n",
    "    # Tokenize the texts\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Use the pooled output for sentence embeddings\n",
    "    embeddings = model_output.pooler_output\n",
    "    return embeddings\n",
    "\n",
    "# Assuming 'all_texts' contains our user_text_prompt, smart_story_descriptions, and image labels combined\n",
    "embeddings = encode_texts(all_texts)\n",
    "\n",
    "# Compute cosine similarity between embeddings\n",
    "# Assuming we have the embeddings for the user_text_prompt and all image labels\n",
    "cosine_similarities = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1:].detach().numpy())\n",
    "\n",
    "# Find the most similar image\n",
    "max_similarity_index = cosine_similarities.argmax()\n",
    "most_similar_image_name = list(labels_dict.keys())[max_similarity_index]\n",
    "\n",
    "# Display the most similar image name and its similarity score\n",
    "most_similar_image_name, cosine_similarities[0, max_similarity_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
