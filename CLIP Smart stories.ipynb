{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\Hussein\\AppData\\Local\\Temp\\ipykernel_13552\\1133486942.py:8: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  BASE_IMG_DIR = \"Example Data-20240208T214429Z-001\\Example Data\\exported\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine  Similarity Score -1 to 1\n",
      "1 indicates identical directionality (very similar),\n",
      "0 indicates orthogonality (not similar),\n",
      "and -1 indicates opposite directionality (very dissimilar).\n",
      "Cosine similarity score for user text prompt: 0.2476407289505005\n",
      "Cosine similarity score for smart story description: 0.18213330209255219\n"
     ]
    }
   ],
   "source": [
    "# Load the CLIP model onto the CPU\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n",
    "\n",
    "# Load the NLP model (spaCy)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Setting up a base directory for the images \n",
    "BASE_IMG_DIR = \"Example Data-20240208T214429Z-001\\Example Data\\exported\"\n",
    "image_name = \"IMG_3155.JPG\"\n",
    "# Prepare the image\n",
    "image_path = f\"{BASE_IMG_DIR}/{image_name}\"\n",
    "\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).to('cpu')\n",
    "\n",
    "# Accept user input for text prompt and smart story description\n",
    "user_text_prompt = \"a photo of people standing outside\"\n",
    "smart_story_description = \"Three individuals outdoors, smiling at the camera and pointing at a park bench.\"\n",
    "\n",
    "# Process the texts with spaCy for named entity recognition (NER)\n",
    "user_doc = nlp(user_text_prompt)\n",
    "story_doc = nlp(smart_story_description)\n",
    "\n",
    "# Extract entities, or use the original text if no entities are found\n",
    "user_entities = ' '.join([ent.text for ent in user_doc.ents]) if user_doc.ents else user_text_prompt\n",
    "story_entities = ' '.join([ent.text for ent in story_doc.ents]) if story_doc.ents else smart_story_description\n",
    "\n",
    "# Tokenize the refined texts\n",
    "text_inputs = clip.tokenize([user_entities, story_entities]).to('cpu')\n",
    "\n",
    "# Calculate the features with CLIP\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    # Normalize the features to unit vectors\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarities = torch.matmul(image_features, text_features.T).cpu().numpy().flatten()\n",
    "\n",
    "# Print the cosine similarity scores\n",
    "print(\"Cosine  Similarity Score -1 to 1\")\n",
    "print(\"1 indicates identical directionality (very similar),\")\n",
    "print(\"0 indicates orthogonality (not similar),\")\n",
    "print(\"and -1 indicates opposite directionality (very dissimilar).\"  )\n",
    "print(f\"Cosine similarity score for user text prompt: {cosine_similarities[0]}\")\n",
    "print(f\"Cosine similarity score for smart story description: {cosine_similarities[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 54\u001b[0m\n\u001b[0;32m     47\u001b[0m coco_dataset \u001b[38;5;241m=\u001b[39m LoadCocoDataset(\n\u001b[0;32m     48\u001b[0m     image_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/coco128/images\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     49\u001b[0m     label_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/coco128/labels/train2017\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     50\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Initialize DataLoader\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class LoadCocoDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # List all image files in the directory\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load label\n",
    "        label_path = os.path.join(self.label_dir, self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        # Assuming the labels are space-separated values: class x_center y_center width height\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    labels.append(np.fromstring(line, sep=' '))\n",
    "\n",
    "        labels = np.array(labels)  # Convert to a NumPy array\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Define your transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize your dataset\n",
    "coco_dataset = LoadCocoDataset(\n",
    "    image_dir='datasets/coco128/images',\n",
    "    label_dir='datasets/coco128/labels/train2017',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader = DataLoader(coco_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_loader\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images, labels)\u001b[38;5;66;03m# - images: a batch of images\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# - labels: the corresponding labels for each image\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for images, labels in data_loader:\n",
    "    print(images, labels)# - images: a batch of images\n",
    "    # - labels: the corresponding labels for each image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hussein\\AppData\\Local\\Temp\\ipykernel_14576\\2903431629.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Label'] = filtered_df['Instance Count'].astype(str) + ' ' + filtered_df['Label']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Name</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1) 18380579401063495.JPG</td>\n",
       "      <td>1 Motor Bike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1) @GreyCupFestival - 109th Grey Cup.jpeg</td>\n",
       "      <td>2 Adult, 1 Female, 7 Person, 1 Woman, 1 Male, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10) 17887803224903630.jpeg</td>\n",
       "      <td>0 Animal, 1 Horse, 0 Horseback Riding, 0 Leisu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(12) 17985809330117499.jpeg</td>\n",
       "      <td>0 Fun, 0 Vacation, 1 Person, 0 Road Trip, 1 He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(13) 18013990822817757.jpeg</td>\n",
       "      <td>0 Clothing, 0 Coat, 0 Jacket, 0 Vest, 0 Shirt,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Image Name  \\\n",
       "0                   (1) 18380579401063495.JPG   \n",
       "1  (1) @GreyCupFestival - 109th Grey Cup.jpeg   \n",
       "2                 (10) 17887803224903630.jpeg   \n",
       "3                 (12) 17985809330117499.jpeg   \n",
       "4                 (13) 18013990822817757.jpeg   \n",
       "\n",
       "                                               Label  \n",
       "0                                       1 Motor Bike  \n",
       "1  2 Adult, 1 Female, 7 Person, 1 Woman, 1 Male, ...  \n",
       "2  0 Animal, 1 Horse, 0 Horseback Riding, 0 Leisu...  \n",
       "3  0 Fun, 0 Vacation, 1 Person, 0 Road Trip, 1 He...  \n",
       "4  0 Clothing, 0 Coat, 0 Jacket, 0 Vest, 0 Shirt,...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Example Data-20240208T214429Z-001/Example Data/ImageLabels.xlsx', usecols=['Image Name', 'Confidence', 'Instance Count', 'Label'])\n",
    "\n",
    "\n",
    "\n",
    "# Append the instance counts >=1 to the label, e.g: '1 Human'\n",
    "\n",
    "# Filter based on the criteria\n",
    "filtered_df = df[(df['Confidence'] >= 70)]\n",
    "\n",
    "# Append the instance counts to the label, e.g: '1 Human'\n",
    "filtered_df['Label'] = filtered_df['Instance Count'].astype(str) + ' ' + filtered_df['Label']\n",
    "\n",
    "# Aggregate labels for each image\n",
    "aggregated_labels = filtered_df.groupby('Image Name')['Label'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "\n",
    "# Display the aggregated dataframe\n",
    "aggregated_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(1) 18380579401063495.JPG': '1 Motor Bike',\n",
       " '(1) @GreyCupFestival - 109th Grey Cup.jpeg': '2 Adult, 1 Female, 7 Person, 1 Woman, 1 Male, 1 Man, 1 Helmet, 0 Crowd, 0 Clothing, 1 Coat, 0 Footwear, 1 Shoe, 0 Face, 0 Head, 0 Accessories, 0 Bag, 1 Handbag, 1 Glasses',\n",
       " '(10) 17887803224903630.jpeg': '0 Animal, 1 Horse, 0 Horseback Riding, 0 Leisure Activities, 0 Mammal, 1 Person, 1 Adult, 1 Female, 1 Woman',\n",
       " '(12) 17985809330117499.jpeg': '0 Fun, 0 Vacation, 1 Person, 0 Road Trip, 1 Helmet, 0 Nature, 0 Outdoors, 0 Sky, 1 Motorcycle, 0 Transportation, 0 Vehicle',\n",
       " '(13) 18013990822817757.jpeg': '0 Clothing, 0 Coat, 0 Jacket, 0 Vest, 0 Shirt, 0 T-Shirt, 0 Document, 0 Receipt, 0 Text, 0 Id Cards, 3 Passport, 0 Symbol, 0 Face, 0 Head, 1 Person, 1 Credit Card, 1 Business Card, 0 Paper',\n",
       " '(14) 17993584322154200.jpeg': '0 Advertisement, 2 Person, 0 Photography, 0 Animal, 6 Bird, 0 Nature, 0 Outdoors, 0 Poster, 0 Clothing, 1 Glove, 0 Wildlife, 0 Footwear, 2 Shoe, 1 Adult, 1 Bride, 1 Female, 0 Wedding, 1 Woman, 0 Mammal',\n",
       " '(15) 18346855723078911.jpeg': '0 Advertisement, 0 Poster',\n",
       " '(16) 18379894042056715.jpeg': '2 Person, 0 Photobombing, 0 Bottle, 2 Shaker, 1 Adult, 1 Female, 1 Woman, 1 Baseball Cap, 0 Cap, 0 Clothing, 0 Hat, 0 Face, 0 Head',\n",
       " '(17) 17876557646942156.jpeg': '0 People, 3 Person, 2 Adult, 2 Male, 2 Man, 0 Clothing, 1 Glove, 1 Helmet, 0 Footwear, 4 Shoe, 0 Shorts, 0 American Football, 0 Football, 0 Playing American Football, 0 Sport',\n",
       " '(18) 17956303754673865.jpeg': '1 Helmet, 0 American Football, 0 Football, 1 Person, 0 Playing American Football, 0 Sport, 1 Adult, 1 Male, 1 Man, 0 Clothing, 2 Glove, 0 People',\n",
       " '(19) 18027245935589987.jpeg': '1 Helmet, 1 Adult, 1 Male, 1 Man, 1 Person, 0 Clothing, 2 Glove, 0 American Football, 0 Football, 0 Playing American Football, 0 Sport, 0 Advertisement, 0 Footwear, 2 Shoe',\n",
       " '(2) 18040499875507660.jpeg': '0 Nature, 0 Outdoors, 0 Adventure, 0 Leisure Activities, 2 Person, 0 Snow, 0 Snowboarding, 0 Sport, 1 Boy, 1 Child, 1 Male, 0 Clothing, 1 Glove, 1 Helmet',\n",
       " '(2) @VanArchives - Vancouver Archives.jpeg': '0 Face, 0 Head, 14 Person, 0 Photography, 0 Portrait, 0 Lady, 3 Adult, 2 Male, 2 Man, 0 Clothing, 0 Coat, 0 Crowd, 1 Female, 1 Woman, 0 People, 0 Jacket, 0 Audience',\n",
       " '(2) ball-park-brand-mflmvznfdq8-unsplash.jpeg': '0 Fire, 0 Flame, 4 Adult, 3 Female, 5 Person, 3 Woman, 1 Bonfire, 1 Male, 1 Man, 0 Face, 0 Head, 0 Happy, 0 Clothing, 0 Footwear, 1 Shoe',\n",
       " '(3) 18005670805793076.jpeg': '0 Helmet, 0 Machine, 0 Motor, 0 Spoke, 0 Clothing, 0 Hardhat, 0 Crash Helmet, 1 Adult, 1 Male, 1 Man, 2 Person, 1 Motorcycle, 0 Transportation, 0 Vehicle, 4 Car, 3 Wheel, 1 Glove, 0 Tire',\n",
       " '(3) @NewEraCanada - New Era Canada.jpeg': '0 Baseball Cap, 0 Cap, 0 Clothing, 2 Hat, 0 Advertisement, 0 Poster, 1 QR Code',\n",
       " '(4) 17978197394393810.jpeg': '4 Adult, 4 Male, 4 Man, 10 Person, 2 Motorcycle, 0 Transportation, 0 Vehicle, 0 Machine, 2 Wheel, 0 Fun, 0 Vacation, 0 Officer, 0 Police Officer, 0 Face, 0 Head, 1 E-scooter',\n",
       " '(4) @betregal - BetRegal.jpeg': '0 Advertisement, 0 Poster, 1 Scoreboard, 0 Text',\n",
       " '(5) @Stats_Junkie - Chris ðŸ‡¨ðŸ‡¦ðŸˆ Stats Junkie.png': '0 Chart, 0 Plot, 0 Page, 0 Text, 0 Number, 0 Symbol, 0 Measurements',\n",
       " '(6) 18030065674525036.jpeg': '0 Text, 0 Advertisement, 0 Number, 0 Symbol',\n",
       " '(6) @Tara_Marie29 - Tara-Marie Hall.jpeg': '0 Ball, 0 Rugby, 1 Rugby Ball, 0 Sport, 0 Face, 0 Head, 1 Person, 0 Photography, 0 Portrait, 1 Boy, 1 Child, 1 Male, 0 American Football, 1 American Football (Ball), 0 Football, 0 Playing American Football, 0 Helmet, 0 People',\n",
       " '(7) @Cdn_Turkey - Canadian Turkey.jpeg': '0 Food, 0 Food Presentation, 1 Pizza, 0 Cutlery, 1 Spoon, 0 Bread',\n",
       " '(8) 17963380910618789.jpeg': '0 Road, 0 Tarmac, 0 Person, 0 Walking, 0 Freeway, 0 Highway, 0 Clothing, 0 Footwear, 1 Shoe, 0 Sneaker',\n",
       " '(8) @fisherwavy - Fisher Wavy.jpeg': '0 Bbq, 0 Cooking, 0 Food, 0 Grilling, 0 Advertisement, 1 Person, 2 Car, 0 Transportation, 0 Vehicle, 0 Device, 0 Electrical Device',\n",
       " 'IMG_0866 Large.jpeg': '11 Person, 0 Outdoors',\n",
       " 'IMG_2318 Large.jpeg': '3 Adult, 1 Female, 13 Person, 1 Woman, 0 People, 0 Crowd, 0 Architecture, 1 Building, 2 Male, 2 Man, 2 Plant, 0 Indoors, 0 Face, 0 Head, 0 Audience',\n",
       " 'IMG_2334 Large.jpeg': '0 Architecture, 3 Building, 0 Office Building, 0 School, 0 Factory, 0 Housing, 0 College',\n",
       " 'IMG_2345 Large.jpeg': '0 Conversation, 3 Person, 0 Interview, 2 Adult, 2 Male, 2 Man, 1 Plant, 0 Clothing, 0 Footwear, 3 Shoe, 0 Crowd, 0 Computer, 0 Electronics, 1 Laptop, 0 Pc, 1 Chair, 0 Furniture, 0 Audience, 0 Indoors, 0 Face, 0 Head',\n",
       " 'IMG_3155.JPG': '0 Clothing, 3 Coat, 2 Adult, 2 Male, 2 Man, 2 Person, 2 Glove, 0 Advertisement, 1 Jacket, 2 Car, 0 Transportation, 0 Vehicle, 0 Face, 0 Head',\n",
       " 'IMG_3386 Large.jpeg': '0 Airfield, 0 Airport, 0 Aircraft, 1 Airplane, 0 Transportation, 0 Vehicle, 0 Airliner',\n",
       " 'IMG_3614 Large.jpeg': '0 Architecture, 0 Building, 0 Outdoors, 0 Shelter, 0 City, 0 Summer, 0 Road, 0 Street, 0 Urban, 0 Neighborhood, 0 Path, 12 Person, 0 Garden, 0 Nature, 0 Arbour, 0 Hotel, 0 Walkway, 0 Accessories, 0 Bag, 1 Handbag, 0 Plant, 0 Tree, 0 Light, 1 Traffic Light, 0 Vegetation, 0 Countryside, 0 Hut, 0 Rural'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to a dictionary for ease of access\n",
    "image_labels_dict = aggregated_labels.set_index('Image Name')['Label'].to_dict()\n",
    "\n",
    "# Displaying the dictionary\n",
    "image_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33, 169),\n",
       " array(['10', '11', '12', '13', '14', 'accessories', 'activities', 'adult',\n",
       "        'adventure', 'advertisement'], dtype=object))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example user inputs\n",
    "user_text_prompt = \"I went snow boarding the mountains\"\n",
    "smart_story_description = \"Rockey Mountains\"\n",
    "\n",
    "# Step 4: Tokenize the user_text_prompt, smart_story_descriptions, and the image labels.\n",
    "\n",
    "# Combine the user inputs and the image labels into a single list for tokenization\n",
    "texts_to_tokenize = [user_text_prompt, smart_story_description] + list(image_labels_dict.values())\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_texts = vectorizer.fit_transform(texts_to_tokenize)\n",
    "\n",
    "# Convert the tokenized texts to an array for easier viewing and further processing\n",
    "tokenized_texts_array = tokenized_texts.toarray()\n",
    "\n",
    "# Show the shape of the array to confirm the tokenization\n",
    "tokenized_texts_array.shape, vectorizer.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(2) 18040499875507660.jpeg', 0.11547005383792514)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# and that the first two rows correspond to user_text_prompt and smart_story_description respectively\n",
    "\n",
    "# Extract vectors for user inputs\n",
    "user_text_vector = tokenized_texts_array[0].reshape(1, -1)  # User text prompt\n",
    "story_desc_vector = tokenized_texts_array[1].reshape(1, -1)  # Smart story description\n",
    "\n",
    "# Calculate cosine similarity between user inputs and all image labels\n",
    "cos_sim_user_text = cosine_similarity(user_text_vector, tokenized_texts_array[2:])  # Skipping the first two\n",
    "cos_sim_story_desc = cosine_similarity(story_desc_vector, tokenized_texts_array[2:])\n",
    "\n",
    "# The result is a list of similarity scores between the user inputs and each image label vector\n",
    "# Let's assume we want to find the image labels most similar to the user_text_prompt\n",
    "most_similar_index = cos_sim_user_text.argmax()  # This gets the index of the highest similarity score\n",
    "most_similar_image = list(image_labels_dict.keys())[most_similar_index]  # Retrieve the corresponding image name\n",
    "\n",
    "most_similar_image, cos_sim_user_text[0, most_similar_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fast Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Using cached fasttext-0.9.2.tar.gz (68 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\python312\\lib\\site-packages (from fasttext) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\python312\\lib\\site-packages (from fasttext) (69.1.1)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from fasttext) (1.26.4)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py): started\n",
      "  Building wheel for fasttext (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for fasttext\n",
      "Failed to build fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— python setup.py bdist_wheel did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [43 lines of output]\n",
      "      C:\\Python312\\Lib\\site-packages\\setuptools\\dist.py:472: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2024-Sep-26, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self.warn_dash_deprecation(opt, section)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-312\n",
      "      creating build\\lib.win-amd64-cpython-312\\fasttext\n",
      "      copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-cpython-312\\fasttext\n",
      "      copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-cpython-312\\fasttext\n",
      "      creating build\\lib.win-amd64-cpython-312\\fasttext\\util\n",
      "      copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-cpython-312\\fasttext\\util\n",
      "      copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-cpython-312\\fasttext\\util\n",
      "      creating build\\lib.win-amd64-cpython-312\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-cpython-312\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-cpython-312\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\fasttext\\tests\n",
      "      running build_ext\n",
      "      building 'fasttext_pybind' extension\n",
      "      creating build\\temp.win-amd64-cpython-312\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\python\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\python\\fasttext_module\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\python\\fasttext_module\\fasttext\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\python\\fasttext_module\\fasttext\\pybind\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\src\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Python312\\Lib\\site-packages\\pybind11\\include -IC:\\Python312\\Lib\\site-packages\\pybind11\\include -Isrc -IC:\\Python312\\include -IC:\\Python312\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\" /EHsc /Tppython/fasttext_module/fasttext/pybind/fasttext_pybind.cc /Fobuild\\temp.win-amd64-cpython-312\\Release\\python/fasttext_module/fasttext/pybind/fasttext_pybind.obj /EHsc /DVERSION_INFO=\\\\\\\"0.9.2\\\\\\\"\n",
      "      fasttext_pybind.cc\n",
      "      C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\\yvals.h(12): fatal error C1083: Cannot open include file: 'crtdbg.h': No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.29.30133\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for fasttext\n",
      "ERROR: Could not build wheels for fasttext, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install fasttext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load pre-trained FastText model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# fasttext.util.download_model('en', if_exists='ignore')  # Uncomment if you need to download the model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "! pip install fasttext\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# Load pre-trained FastText model\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # Uncomment if you need to download the model\n",
    "ft_model = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "def text_to_embedding(text, model):\n",
    "    \"\"\"\n",
    "    Converts a text to an embedding by averaging the embeddings of the words in the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    embeddings = [model.get_word_vector(word) for word in words]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())\n",
    "\n",
    "# Assuming aggregated_labels is your DataFrame from previous steps\n",
    "# Convert each label into an embedding\n",
    "aggregated_labels['Embedding'] = aggregated_labels['Label'].apply(lambda x: text_to_embedding(x, ft_model))\n",
    "\n",
    "# Example user input\n",
    "user_text_prompt = \"This my cat is he is the best, cats are better than dogs\"\n",
    "user_embedding = text_to_embedding(user_text_prompt, ft_model)\n",
    "\n",
    "# Calculate cosine similarity between user input and image labels\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Adding a similarity score for each image label\n",
    "aggregated_labels['Similarity'] = aggregated_labels['Embedding'].apply(lambda x: cosine_similarity(x, user_embedding))\n",
    "\n",
    "# Find the most similar images\n",
    "most_similar_images = aggregated_labels.sort_values(by='Similarity', ascending=False).head()\n",
    "\n",
    "# Display the most similar images\n",
    "most_similar_images[['Image Name', 'Label', 'Similarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
