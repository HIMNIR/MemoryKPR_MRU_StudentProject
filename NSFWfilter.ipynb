{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries, in our case we are using pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch  # PyTorch library for deep learning\n",
    "from torch import nn, optim  # Neural network modules and optimization algorithms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split  # Tools for handling datasets\n",
    "from torchvision import transforms, models  # Vision-related utilities including pre-trained models\n",
    "from matplotlib import pyplot as plt  # Plotting library for visualization\n",
    "from PIL import Image  # Python Imaging Library for image processing\n",
    "from tqdm import tqdm  # Progress bar for tracking iterations\n",
    "import os  # Operating system utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be sure to change the directory of images, and look at instructions below on how to run the model without retraining the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables\n",
    "\n",
    "# Path to the folder containing images\n",
    "data_folder = './images'\n",
    "\n",
    "# Number of classes in the dataset\n",
    "classes = 5\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "lr = 0.001\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 32\n",
    "\n",
    "# Number of epochs for training\n",
    "epochs = 40\n",
    "\n",
    "# Size of the input images (assumed to be square)\n",
    "image_size = 224\n",
    "\n",
    "# Device selection based on GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loading the VGG16 model with default weights\n",
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(model):\n",
    "    # Function to freeze the parameters of a given model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Getting the number of input features for the last layer of the model's classifier\n",
    "num_features = model.classifier[-1].in_features\n",
    "\n",
    "# Flag to determine if we want to freeze the features of the model\n",
    "freeze_features = True\n",
    "\n",
    "# If freeze_features flag is set to True, freeze the parameters of the model\n",
    "if freeze_features:\n",
    "    freeze(model)\n",
    "\n",
    "# Updating the last layer of the model's classifier to output classes specific to our problem\n",
    "model.classifier[-1] = nn.Linear(num_features, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: USE THIS AT YOUR OWN RISK!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "def visualize(image, label):\n",
    "  image = torch.permute(image, (2, 1, 0)).numpy()\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(str(label))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "  def __init__(self, source_dir, transform = None):\n",
    "\n",
    "    #Variables to keep track of\n",
    "    self.labelDictionary = {}\n",
    "    self.images = []\n",
    "    self.labels = []\n",
    "    self.transform = transform\n",
    "    self.toTensor = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    #If the source directory provided isn't a directory, return -1\n",
    "    if os.path.isdir(source_dir) == False:\n",
    "      print(f\"{source_dir} not a valid directory\")\n",
    "      return -1\n",
    "\n",
    "    i = 0\n",
    "    #Go through the files inside the source_dir\n",
    "    for dir in os.listdir(source_dir):\n",
    "      for file in os.listdir(os.path.join(source_dir, dir)):\n",
    "\n",
    "        if i not in self.labelDictionary:\n",
    "          self.labelDictionary[i] = dir\n",
    "\n",
    "        try:\n",
    "          Image.open(os.path.join(source_dir, dir, file))\n",
    "        except:\n",
    "          continue\n",
    "      \n",
    "        self.images.append(os.path.join(source_dir, dir, file))\n",
    "        self.labels.append(i)\n",
    "      i += 1\n",
    "\n",
    "  #Function that returns the size of the dataset\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  #Function that returns the ith image and label after converting the image\n",
    "  def __getitem__(self, i):\n",
    "\n",
    "    #Opening an Image\n",
    "    image = Image.open(self.images[i])\n",
    "  \n",
    "    label = self.labels[i]\n",
    "\n",
    "    #All the transformations\n",
    "    if self.transform is not None:\n",
    "      image = self.transform(image)\n",
    "\n",
    "    img = self.toTensor(image)\n",
    "    if img.shape[0] != 3:\n",
    "      rgbimg = Image.new(\"RGB\", image.size)\n",
    "      rgbimg.paste(image)\n",
    "      img = self.toTensor(rgbimg)\n",
    "\n",
    "    # image = image.float()\n",
    "    return img, label\n",
    "  \n",
    "  def getLabels(self):\n",
    "    return self.labels\n",
    "  \n",
    "  def getDictionary(self):\n",
    "    return self.labelDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utsav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (98130452 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = CreateDataset(source_dir=data_folder, transform=transform)\n",
    "train_dataset, valid_dataset, test_dataset = random_split(data, [0.8, 0.1, 0.1])\n",
    "\n",
    "temp_train = train_dataset[0]\n",
    "temp_valid = valid_dataset[0]\n",
    "temp_test = test_dataset[0]\n",
    "\n",
    "temp = data.getLabels()\n",
    "# print(data.getDictionary()[0], temp.count(0))\n",
    "\n",
    "# visualize(temp_train[0], temp_train[1])\n",
    "# visualize(temp_valid[0], temp_valid[1])\n",
    "# visualize(temp_test[0], temp_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = {\n",
    "    'train': train_dataset,\n",
    "    'valid': valid_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n",
    "loader = {\n",
    "    phase: DataLoader(ds, batch_size=batch_size, shuffle=(phase=='train'))\n",
    "    for phase, ds in phases.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device, criterion, mode='validation'):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "  \n",
    "    # Initialize variables to track total data, correct predictions, and total loss\n",
    "    total = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "  \n",
    "    # Iterate through the data loader\n",
    "    for i, (images, labels) in enumerate(loader[mode]):\n",
    "\n",
    "        # Move images and labels to the specified device (CPU or GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        # Disable gradient computation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "      \n",
    "            # Accumulate the total loss\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Accumulate the total number of data points\n",
    "            total += images.size(0)\n",
    "            \n",
    "            # Calculate the number of correct predictions\n",
    "            _, predictions = outputs.max(1)\n",
    "            total_correct += (labels == predictions).sum()\n",
    "  \n",
    "    # Calculate average loss and accuracy\n",
    "    loss = total_loss / total\n",
    "    accuracy = total_correct / total\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f'{mode} epoch {epoch}: Loss({loss:6.4f}) Accuracy ({accuracy:6.4f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:31<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0: Loss(0.7539) Accuracy (0.7161)\n",
      "valid epoch 0: Loss(0.5134) Accuracy (0.7974)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:32<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: Loss(0.6186) Accuracy (0.7737)\n",
      "valid epoch 1: Loss(0.4950) Accuracy (0.8066)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:31<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 2: Loss(0.5838) Accuracy (0.7833)\n",
      "valid epoch 2: Loss(0.4804) Accuracy (0.8291)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 3: Loss(0.5688) Accuracy (0.7864)\n",
      "valid epoch 3: Loss(0.4579) Accuracy (0.8238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:27<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 4: Loss(0.5452) Accuracy (0.7922)\n",
      "valid epoch 4: Loss(0.4488) Accuracy (0.8185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 5: Loss(0.5405) Accuracy (0.7970)\n",
      "valid epoch 5: Loss(0.4529) Accuracy (0.8318)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 6: Loss(0.5470) Accuracy (0.8003)\n",
      "valid epoch 6: Loss(0.4521) Accuracy (0.8278)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:38<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 7: Loss(0.5312) Accuracy (0.8015)\n",
      "valid epoch 7: Loss(0.4500) Accuracy (0.8278)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:31<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 8: Loss(0.5109) Accuracy (0.8083)\n",
      "valid epoch 8: Loss(0.4593) Accuracy (0.8238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:31<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 9: Loss(0.5306) Accuracy (0.7995)\n",
      "valid epoch 9: Loss(0.4402) Accuracy (0.8305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 10: Loss(0.5069) Accuracy (0.8099)\n",
      "valid epoch 10: Loss(0.4626) Accuracy (0.8199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 11: Loss(0.5139) Accuracy (0.8094)\n",
      "valid epoch 11: Loss(0.4504) Accuracy (0.8185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 12: Loss(0.4955) Accuracy (0.8162)\n",
      "valid epoch 12: Loss(0.4449) Accuracy (0.8252)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:29<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 13: Loss(0.5014) Accuracy (0.8089)\n",
      "valid epoch 13: Loss(0.4374) Accuracy (0.8331)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 14: Loss(0.5027) Accuracy (0.8091)\n",
      "valid epoch 14: Loss(0.4510) Accuracy (0.8278)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:27<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 15: Loss(0.4930) Accuracy (0.8177)\n",
      "valid epoch 15: Loss(0.4371) Accuracy (0.8252)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:27<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 16: Loss(0.4969) Accuracy (0.8137)\n",
      "valid epoch 16: Loss(0.4777) Accuracy (0.8106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 17: Loss(0.4955) Accuracy (0.8141)\n",
      "valid epoch 17: Loss(0.4517) Accuracy (0.8159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 18: Loss(0.5022) Accuracy (0.8122)\n",
      "valid epoch 18: Loss(0.4429) Accuracy (0.8318)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 19: Loss(0.4889) Accuracy (0.8104)\n",
      "valid epoch 19: Loss(0.4226) Accuracy (0.8411)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 20: Loss(0.4868) Accuracy (0.8149)\n",
      "valid epoch 20: Loss(0.4379) Accuracy (0.8291)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:29<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 21: Loss(0.4874) Accuracy (0.8195)\n",
      "valid epoch 21: Loss(0.4517) Accuracy (0.8278)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 22: Loss(0.4938) Accuracy (0.8151)\n",
      "valid epoch 22: Loss(0.4479) Accuracy (0.8278)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 23: Loss(0.4796) Accuracy (0.8182)\n",
      "valid epoch 23: Loss(0.4739) Accuracy (0.8252)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 24: Loss(0.4738) Accuracy (0.8243)\n",
      "valid epoch 24: Loss(0.4327) Accuracy (0.8371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 25: Loss(0.4633) Accuracy (0.8240)\n",
      "valid epoch 25: Loss(0.4241) Accuracy (0.8371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 26: Loss(0.4852) Accuracy (0.8137)\n",
      "valid epoch 26: Loss(0.4274) Accuracy (0.8411)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 27: Loss(0.4806) Accuracy (0.8195)\n",
      "valid epoch 27: Loss(0.4394) Accuracy (0.8305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:27<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 28: Loss(0.4728) Accuracy (0.8207)\n",
      "valid epoch 28: Loss(0.4214) Accuracy (0.8450)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 29: Loss(0.4715) Accuracy (0.8233)\n",
      "valid epoch 29: Loss(0.4419) Accuracy (0.8331)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:27<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 30: Loss(0.4732) Accuracy (0.8223)\n",
      "valid epoch 30: Loss(0.4557) Accuracy (0.8344)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 31: Loss(0.4645) Accuracy (0.8251)\n",
      "valid epoch 31: Loss(0.4540) Accuracy (0.8252)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:27<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 32: Loss(0.4585) Accuracy (0.8261)\n",
      "valid epoch 32: Loss(0.4212) Accuracy (0.8424)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 33: Loss(0.4718) Accuracy (0.8235)\n",
      "valid epoch 33: Loss(0.4203) Accuracy (0.8464)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 34: Loss(0.4633) Accuracy (0.8199)\n",
      "valid epoch 34: Loss(0.4423) Accuracy (0.8331)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 35: Loss(0.4612) Accuracy (0.8242)\n",
      "valid epoch 35: Loss(0.4353) Accuracy (0.8371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 36: Loss(0.4699) Accuracy (0.8212)\n",
      "valid epoch 36: Loss(0.4486) Accuracy (0.8318)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 37: Loss(0.4619) Accuracy (0.8220)\n",
      "valid epoch 37: Loss(0.4261) Accuracy (0.8424)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:28<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 38: Loss(0.4718) Accuracy (0.8197)\n",
      "valid epoch 38: Loss(0.4266) Accuracy (0.8411)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [02:27<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 39: Loss(0.4599) Accuracy (0.8246)\n",
      "valid epoch 39: Loss(0.4338) Accuracy (0.8344)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize variables to track total data, correct predictions, and total loss for this epoch\n",
    "    total = 0\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate through the training data loader\n",
    "    for i, (images, labels) in tqdm(enumerate(loader['train']), total=len(loader['train'])):\n",
    "\n",
    "        # Move images and labels to the specified device (CPU or GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Accumulate the total number of data points\n",
    "        total += images.size(0)\n",
    "        \n",
    "        # Calculate the number of correct predictions\n",
    "        _, predictions = outputs.max(1)\n",
    "        total_correct += (predictions == labels).sum()\n",
    "\n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    accuracy = total_correct / total\n",
    "    loss = total_loss / total\n",
    "\n",
    "    # Print training metrics for this epoch\n",
    "    print(f'Train epoch {epoch}: Loss({loss:6.4f}) Accuracy ({accuracy:6.4f})')\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    evaluate(model, loader, device, criterion, mode='valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch 39: Loss(0.5200) Accuracy (0.8106)\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, loader, device, criterion, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"2nd-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load the Trained Model:** Load the trained VGG16 model along with its weights using `torch.load(path_to_model)` function.\n",
    "\n",
    "\n",
    "# Load the trained VGG16 model\n",
    "```\n",
    "model = torch.load('path_to_your_trained_model.pth')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# Transform the images to fit img size\n",
    "```\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "```\n",
    "# You only have the run the first 3 cells before you call the model.eval method \n",
    "\n",
    "```\n",
    " The labels used in training are [drawings,hentai,neutral,porn,sexy]\n",
    " ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
