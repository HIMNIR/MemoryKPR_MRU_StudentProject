{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hussein\\AppData\\Local\\Temp\\ipykernel_11852\\4204607573.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "c:\\Python312\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Test Pretrained Inception Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predicted labels for (1) 18380579401063495.jpg (top-3):\n",
      "1: motor_scooter (90.88%)\n",
      "2: moped (1.75%)\n",
      "3: snowmobile (1.61%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Predicted labels for (1) 18380579401063495.png (top-3):\n",
      "1: motor_scooter (89.87%)\n",
      "2: snowmobile (1.72%)\n",
      "3: moped (1.67%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Predicted labels for (1) @GreyCupFestival - 109th Grey Cup.jpeg (top-3):\n",
      "1: stage (58.90%)\n",
      "2: moving_van (2.70%)\n",
      "3: mortarboard (1.68%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Predicted labels for (10) 17887803224903630.jpeg (top-3):\n",
      "1: seashore (39.63%)\n",
      "2: sandbar (7.99%)\n",
      "3: Eskimo_dog (3.33%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Predicted labels for (11) 17997439897932301.png (top-3):\n",
      "1: crash_helmet (60.60%)\n",
      "2: motor_scooter (5.71%)\n",
      "3: moped (3.50%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Predicted labels for (12) 17985809330117499.jpeg (top-3):\n",
      "1: moped (63.87%)\n",
      "2: motor_scooter (16.93%)\n",
      "3: crash_helmet (8.56%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Predicted labels for (13) 18013990822817757.jpeg (top-3):\n",
      "1: jersey (50.91%)\n",
      "2: mailbag (3.02%)\n",
      "3: Christmas_stocking (2.67%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Predicted labels for (14) 17993584322154200.jpeg (top-3):\n",
      "1: cliff (27.65%)\n",
      "2: fountain (6.66%)\n",
      "3: ski (5.71%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Predicted labels for (15) 18346855723078911.jpeg (top-3):\n",
      "1: street_sign (22.71%)\n",
      "2: scoreboard (14.95%)\n",
      "3: packet (5.16%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Predicted labels for (16) 18379894042056715.jpeg (top-3):\n",
      "1: cowboy_hat (19.28%)\n",
      "2: shower_cap (18.05%)\n",
      "3: cowboy_boot (3.09%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Predicted labels for (17) 17876557646942156.jpeg (top-3):\n",
      "1: football_helmet (63.30%)\n",
      "2: ballplayer (1.76%)\n",
      "3: bathing_cap (1.68%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Predicted labels for (18) 17956303754673865.jpeg (top-3):\n",
      "1: football_helmet (48.31%)\n",
      "2: ballplayer (9.04%)\n",
      "3: baseball (2.34%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Predicted labels for (19) 18027245935589987.jpeg (top-3):\n",
      "1: book_jacket (79.81%)\n",
      "2: comic_book (3.82%)\n",
      "3: ski (2.78%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Predicted labels for (2) 18040499875507660.jpeg (top-3):\n",
      "1: ski (74.65%)\n",
      "2: puck (4.28%)\n",
      "3: ski_mask (2.52%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Predicted labels for (2) @VanArchives - Vancouver Archives.jpeg (top-3):\n",
      "1: mortarboard (30.68%)\n",
      "2: academic_gown (6.63%)\n",
      "3: groom (4.23%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Predicted labels for (2) ball-park-brand-mflmvznfdq8-unsplash.jpeg (top-3):\n",
      "1: conch (27.10%)\n",
      "2: torch (14.17%)\n",
      "3: wok (5.40%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Predicted labels for (3) 18005670805793076.jpeg (top-3):\n",
      "1: moped (65.07%)\n",
      "2: crash_helmet (27.53%)\n",
      "3: motor_scooter (3.97%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Predicted labels for (3) @NewEraCanada - New Era Canada.jpeg (top-3):\n",
      "1: jersey (16.18%)\n",
      "2: comic_book (14.81%)\n",
      "3: pickelhaube (4.00%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Predicted labels for (4) 17978197394393810.jpeg (top-3):\n",
      "1: motor_scooter (68.36%)\n",
      "2: moped (28.41%)\n",
      "3: crash_helmet (0.36%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Predicted labels for (4) @betregal - BetRegal.jpeg (top-3):\n",
      "1: web_site (70.63%)\n",
      "2: street_sign (2.32%)\n",
      "3: analog_clock (1.63%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Predicted labels for (5) 18262020151093596.png (top-3):\n",
      "1: crash_helmet (69.02%)\n",
      "2: moped (10.14%)\n",
      "3: motor_scooter (9.70%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Predicted labels for (5) @Stats_Junkie - Chris ðŸ‡¨ðŸ‡¦ðŸˆ Stats Junkie.png (top-3):\n",
      "1: menu (88.63%)\n",
      "2: web_site (1.05%)\n",
      "3: shower_curtain (0.27%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Predicted labels for (6) 18030065674525036.jpeg (top-3):\n",
      "1: web_site (68.65%)\n",
      "2: menu (6.07%)\n",
      "3: slot (0.54%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Predicted labels for (6) @Tara_Marie29 - Tara-Marie Hall.jpeg (top-3):\n",
      "1: basketball (80.41%)\n",
      "2: rugby_ball (8.90%)\n",
      "3: punching_bag (1.81%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Predicted labels for (7) 18006556474787720.png (top-3):\n",
      "1: moped (83.99%)\n",
      "2: crash_helmet (3.17%)\n",
      "3: motor_scooter (2.71%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Predicted labels for (7) @Cdn_Turkey - Canadian Turkey.jpeg (top-3):\n",
      "1: soup_bowl (20.95%)\n",
      "2: plate (17.46%)\n",
      "3: guacamole (8.70%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Predicted labels for (8) 17963380910618789.jpeg (top-3):\n",
      "1: rapeseed (23.73%)\n",
      "2: sandbar (12.63%)\n",
      "3: dam (5.48%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Predicted labels for (8) @fisherwavy - Fisher Wavy.jpeg (top-3):\n",
      "1: web_site (85.94%)\n",
      "2: harvester (0.27%)\n",
      "3: printer (0.22%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Predicted labels for (9) 17895815912854630.png (top-3):\n",
      "1: crash_helmet (37.81%)\n",
      "2: disk_brake (5.77%)\n",
      "3: mountain_bike (5.51%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Predicted labels for IMG_0866 Large.jpeg (top-3):\n",
      "1: stage (53.45%)\n",
      "2: cinema (19.19%)\n",
      "3: scoreboard (12.39%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Predicted labels for IMG_2318 Large.jpeg (top-3):\n",
      "1: restaurant (42.27%)\n",
      "2: stage (20.24%)\n",
      "3: butcher_shop (7.42%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Predicted labels for IMG_2334 Large.jpeg (top-3):\n",
      "1: dome (52.53%)\n",
      "2: palace (7.47%)\n",
      "3: cinema (3.96%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "Predicted labels for IMG_2345 Large.jpeg (top-3):\n",
      "1: stage (9.04%)\n",
      "2: jigsaw_puzzle (8.76%)\n",
      "3: grand_piano (5.27%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Predicted labels for IMG_3155.JPG (top-3):\n",
      "1: park_bench (3.52%)\n",
      "2: limousine (2.55%)\n",
      "3: abaya (2.07%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Predicted labels for IMG_3386 Large.jpeg (top-3):\n",
      "1: freight_car (71.37%)\n",
      "2: submarine (9.20%)\n",
      "3: breakwater (3.24%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Predicted labels for IMG_3614 Large.jpeg (top-3):\n",
      "1: patio (34.88%)\n",
      "2: thatch (12.06%)\n",
      "3: lakeside (6.05%)\n",
      "\n",
      "Filename: (1) 18380579401063495.jpg\n",
      "Predicted labels (top-3):\n",
      "1: motor_scooter (90.88%)\n",
      "2: moped (1.75%)\n",
      "3: snowmobile (1.61%)\n",
      "\n",
      "Filename: (1) 18380579401063495.png\n",
      "Predicted labels (top-3):\n",
      "1: motor_scooter (89.87%)\n",
      "2: snowmobile (1.72%)\n",
      "3: moped (1.67%)\n",
      "\n",
      "Filename: (1) @GreyCupFestival - 109th Grey Cup.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: stage (58.90%)\n",
      "2: moving_van (2.70%)\n",
      "3: mortarboard (1.68%)\n",
      "\n",
      "Filename: (10) 17887803224903630.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: seashore (39.63%)\n",
      "2: sandbar (7.99%)\n",
      "3: Eskimo_dog (3.33%)\n",
      "\n",
      "Filename: (11) 17997439897932301.png\n",
      "Predicted labels (top-3):\n",
      "1: crash_helmet (60.60%)\n",
      "2: motor_scooter (5.71%)\n",
      "3: moped (3.50%)\n",
      "\n",
      "Filename: (12) 17985809330117499.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: moped (63.87%)\n",
      "2: motor_scooter (16.93%)\n",
      "3: crash_helmet (8.56%)\n",
      "\n",
      "Filename: (13) 18013990822817757.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: jersey (50.91%)\n",
      "2: mailbag (3.02%)\n",
      "3: Christmas_stocking (2.67%)\n",
      "\n",
      "Filename: (14) 17993584322154200.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: cliff (27.65%)\n",
      "2: fountain (6.66%)\n",
      "3: ski (5.71%)\n",
      "\n",
      "Filename: (15) 18346855723078911.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: street_sign (22.71%)\n",
      "2: scoreboard (14.95%)\n",
      "3: packet (5.16%)\n",
      "\n",
      "Filename: (16) 18379894042056715.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: cowboy_hat (19.28%)\n",
      "2: shower_cap (18.05%)\n",
      "3: cowboy_boot (3.09%)\n",
      "\n",
      "Filename: (17) 17876557646942156.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: football_helmet (63.30%)\n",
      "2: ballplayer (1.76%)\n",
      "3: bathing_cap (1.68%)\n",
      "\n",
      "Filename: (18) 17956303754673865.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: football_helmet (48.31%)\n",
      "2: ballplayer (9.04%)\n",
      "3: baseball (2.34%)\n",
      "\n",
      "Filename: (19) 18027245935589987.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: book_jacket (79.81%)\n",
      "2: comic_book (3.82%)\n",
      "3: ski (2.78%)\n",
      "\n",
      "Filename: (2) 18040499875507660.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: ski (74.65%)\n",
      "2: puck (4.28%)\n",
      "3: ski_mask (2.52%)\n",
      "\n",
      "Filename: (2) @VanArchives - Vancouver Archives.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: mortarboard (30.68%)\n",
      "2: academic_gown (6.63%)\n",
      "3: groom (4.23%)\n",
      "\n",
      "Filename: (2) ball-park-brand-mflmvznfdq8-unsplash.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: conch (27.10%)\n",
      "2: torch (14.17%)\n",
      "3: wok (5.40%)\n",
      "\n",
      "Filename: (3) 18005670805793076.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: moped (65.07%)\n",
      "2: crash_helmet (27.53%)\n",
      "3: motor_scooter (3.97%)\n",
      "\n",
      "Filename: (3) @NewEraCanada - New Era Canada.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: jersey (16.18%)\n",
      "2: comic_book (14.81%)\n",
      "3: pickelhaube (4.00%)\n",
      "\n",
      "Filename: (4) 17978197394393810.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: motor_scooter (68.36%)\n",
      "2: moped (28.41%)\n",
      "3: crash_helmet (0.36%)\n",
      "\n",
      "Filename: (4) @betregal - BetRegal.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: web_site (70.63%)\n",
      "2: street_sign (2.32%)\n",
      "3: analog_clock (1.63%)\n",
      "\n",
      "Filename: (5) 18262020151093596.png\n",
      "Predicted labels (top-3):\n",
      "1: crash_helmet (69.02%)\n",
      "2: moped (10.14%)\n",
      "3: motor_scooter (9.70%)\n",
      "\n",
      "Filename: (5) @Stats_Junkie - Chris ðŸ‡¨ðŸ‡¦ðŸˆ Stats Junkie.png\n",
      "Predicted labels (top-3):\n",
      "1: menu (88.63%)\n",
      "2: web_site (1.05%)\n",
      "3: shower_curtain (0.27%)\n",
      "\n",
      "Filename: (6) 18030065674525036.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: web_site (68.65%)\n",
      "2: menu (6.07%)\n",
      "3: slot (0.54%)\n",
      "\n",
      "Filename: (6) @Tara_Marie29 - Tara-Marie Hall.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: basketball (80.41%)\n",
      "2: rugby_ball (8.90%)\n",
      "3: punching_bag (1.81%)\n",
      "\n",
      "Filename: (7) 18006556474787720.png\n",
      "Predicted labels (top-3):\n",
      "1: moped (83.99%)\n",
      "2: crash_helmet (3.17%)\n",
      "3: motor_scooter (2.71%)\n",
      "\n",
      "Filename: (7) @Cdn_Turkey - Canadian Turkey.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: soup_bowl (20.95%)\n",
      "2: plate (17.46%)\n",
      "3: guacamole (8.70%)\n",
      "\n",
      "Filename: (8) 17963380910618789.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: rapeseed (23.73%)\n",
      "2: sandbar (12.63%)\n",
      "3: dam (5.48%)\n",
      "\n",
      "Filename: (8) @fisherwavy - Fisher Wavy.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: web_site (85.94%)\n",
      "2: harvester (0.27%)\n",
      "3: printer (0.22%)\n",
      "\n",
      "Filename: (9) 17895815912854630.png\n",
      "Predicted labels (top-3):\n",
      "1: crash_helmet (37.81%)\n",
      "2: disk_brake (5.77%)\n",
      "3: mountain_bike (5.51%)\n",
      "\n",
      "Filename: IMG_0866 Large.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: stage (53.45%)\n",
      "2: cinema (19.19%)\n",
      "3: scoreboard (12.39%)\n",
      "\n",
      "Filename: IMG_2318 Large.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: restaurant (42.27%)\n",
      "2: stage (20.24%)\n",
      "3: butcher_shop (7.42%)\n",
      "\n",
      "Filename: IMG_2334 Large.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: dome (52.53%)\n",
      "2: palace (7.47%)\n",
      "3: cinema (3.96%)\n",
      "\n",
      "Filename: IMG_2345 Large.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: stage (9.04%)\n",
      "2: jigsaw_puzzle (8.76%)\n",
      "3: grand_piano (5.27%)\n",
      "\n",
      "Filename: IMG_3155.JPG\n",
      "Predicted labels (top-3):\n",
      "1: park_bench (3.52%)\n",
      "2: limousine (2.55%)\n",
      "3: abaya (2.07%)\n",
      "\n",
      "Filename: IMG_3386 Large.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: freight_car (71.37%)\n",
      "2: submarine (9.20%)\n",
      "3: breakwater (3.24%)\n",
      "\n",
      "Filename: IMG_3614 Large.jpeg\n",
      "Predicted labels (top-3):\n",
      "1: patio (34.88%)\n",
      "2: thatch (12.06%)\n",
      "3: lakeside (6.05%)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained InceptionV3 model\n",
    "model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# Function to preprocess the image and predict its label\n",
    "def predict_image_label(img_path):\n",
    "    # Load the image file, resizing it to 299x299 pixels (as required by InceptionV3)\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "    \n",
    "    # Convert the image to a numpy array and add an additional dimension (for batch size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Preprocess the image for the InceptionV3 model\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    # Predict the probabilities across all output classes\n",
    "    predictions = model.predict(img_array)\n",
    "    \n",
    "    # Decode the predictions to get human-readable labels\n",
    "    decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
    "    predictions_list = []\n",
    "    print(f\"Predicted labels for {os.path.basename(img_path)} (top-3):\")\n",
    "    for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
    "        print(f\"{i+1}: {label} ({score*100:.2f}%)\")\n",
    "        predictions_list.append((label, score))\n",
    "    return predictions_list\n",
    "\n",
    "# Path to the folder containing images\n",
    "folder_path = 'Example Data-20240208T214429Z-001/Example Data/exported'\n",
    "\n",
    "predictions_dict = {}\n",
    "# Iterate over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):  # Check for common image file extensions\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        predictions_dict[filename] = predict_image_label(img_path)\n",
    "\n",
    "# Print the filenames and corresponding predicted labels\n",
    "for filename, predicted_labels in predictions_dict.items():\n",
    "    print(f\"\\nFilename: {filename}\")\n",
    "    print(\"Predicted labels (top-3):\")\n",
    "    for i, (label, score) in enumerate(predicted_labels):\n",
    "        print(f\"{i+1}: {label} ({score*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocces the images in the images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    try:\n",
    "        # Load the image file, resizing it to 299x299 pixels (as required by InceptionV3)\n",
    "        img = image.load_img(img_path, target_size=(299, 299))\n",
    "        \n",
    "        # Convert the image to a numpy array\n",
    "        img_array = image.img_to_array(img)\n",
    "        \n",
    "        # Add a dimension to the array for batch size\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Apply the specific preprocessing required by InceptionV3\n",
    "        img_array = preprocess_input(img_array)\n",
    "        \n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_in_folder(folder_path):\n",
    "    preprocessed_images = []\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):  # Check for common image file extensions\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img_array = preprocess_image(img_path)\n",
    "            if img_array is not None:\n",
    "                preprocessed_images.append(img_array)\n",
    "    \n",
    "    return preprocessed_images\n",
    "\n",
    "# Preprocess all images in the specified folder\n",
    "preprocessed_images = preprocess_images_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the image labels from excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = 'Example Data-20240208T214429Z-001/Example Data/Imagelabels.xlsx'\n",
    "\n",
    "\n",
    "# Function to read the Excel file and extract image names and labels\n",
    "def read_labels_from_excel(excel_path):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # the Excel file has columns 'Image Name' and 'Label'\n",
    "    labels_dict = pd.Series(df.Label.values, index=df['Image Name']).to_dict()\n",
    "    \n",
    "    return labels_dict\n",
    "\n",
    "# Call the function and store the result in a variable\n",
    "actual_labels_dict = read_labels_from_excel(excel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "(1) @GreyCupFestival - 109th Grey Cup.jpeg: Predicted Label - stage, Actual Label - Glasses, Similarity - 33%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "(10) 17887803224903630.jpeg: Predicted Label - seashore, Actual Label - Woman, Similarity - 15%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "(12) 17985809330117499.jpeg: Predicted Label - moped, Actual Label - Night, Similarity - 0%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "(13) 18013990822817757.jpeg: Predicted Label - jersey, Actual Label - Paper, Similarity - 36%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "(14) 17993584322154200.jpeg: Predicted Label - cliff, Actual Label - Lion, Similarity - 44%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "(15) 18346855723078911.jpeg: Predicted Label - street_sign, Actual Label - Poster, Similarity - 35%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "(16) 18379894042056715.jpeg: Predicted Label - cowboy_hat, Actual Label - Head, Similarity - 29%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "(17) 17876557646942156.jpeg: Predicted Label - football_helmet, Actual Label - Hand, Similarity - 11%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "(18) 17956303754673865.jpeg: Predicted Label - football_helmet, Actual Label - People, Similarity - 29%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "(19) 18027245935589987.jpeg: Predicted Label - book_jacket, Actual Label - People, Similarity - 24%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "(2) 18040499875507660.jpeg: Predicted Label - ski, Actual Label - Helmet, Similarity - 0%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "(2) @VanArchives - Vancouver Archives.jpeg: Predicted Label - mortarboard, Actual Label - Audience, Similarity - 21%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "(2) ball-park-brand-mflmvznfdq8-unsplash.jpeg: Predicted Label - conch, Actual Label - Shoe, Similarity - 22%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "(3) 18005670805793076.jpeg: Predicted Label - moped, Actual Label - Car Wheel, Similarity - 14%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "(3) @NewEraCanada - New Era Canada.jpeg: Predicted Label - jersey, Actual Label - QR Code, Similarity - 15%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "(4) 17978197394393810.jpeg: Predicted Label - motor_scooter, Actual Label - Shoe, Similarity - 24%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "(4) @betregal - BetRegal.jpeg: Predicted Label - web_site, Actual Label - Text, Similarity - 33%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "(5) @Stats_Junkie - Chris ðŸ‡¨ðŸ‡¦ðŸˆ Stats Junkie.png: Predicted Label - menu, Actual Label - Measurements, Similarity - 38%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "(6) 18030065674525036.jpeg: Predicted Label - web_site, Actual Label - Poster, Similarity - 43%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "(6) @Tara_Marie29 - Tara-Marie Hall.jpeg: Predicted Label - basketball, Actual Label - T-Shirt, Similarity - 24%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "(7) @Cdn_Turkey - Canadian Turkey.jpeg: Predicted Label - soup_bowl, Actual Label - Pancake, Similarity - 12%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "(8) 17963380910618789.jpeg: Predicted Label - rapeseed, Actual Label - Bird, Similarity - 33%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "(8) @fisherwavy - Fisher Wavy.jpeg: Predicted Label - web_site, Actual Label - Appliance, Similarity - 12%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "IMG_0866 Large.jpeg: Predicted Label - stage, Actual Label - Outdoors, Similarity - 15%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "IMG_2318 Large.jpeg: Predicted Label - restaurant, Actual Label - Audience, Similarity - 33%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "IMG_2334 Large.jpeg: Predicted Label - dome, Actual Label - Urban, Similarity - 0%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "IMG_2345 Large.jpeg: Predicted Label - stage, Actual Label - Head, Similarity - 22%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "IMG_3155.JPG: Predicted Label - park_bench, Actual Label - Shoe, Similarity - 14%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "IMG_3386 Large.jpeg: Predicted Label - freight_car, Actual Label - Airliner, Similarity - 32%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "IMG_3614 Large.jpeg: Predicted Label - patio, Actual Label - Rural, Similarity - 20%\n",
      "Average Label Similarity: 22.766666666666666%\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(img_path):\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=(299, 299))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_image_label(img_path):\n",
    "    img_array = preprocess_image(img_path)\n",
    "    if img_array is not None:\n",
    "        predictions = model.predict(img_array)\n",
    "        decoded_predictions = decode_predictions(predictions, top=1)[0]\n",
    "        return decoded_predictions[0][1]  # Return only the top prediction label\n",
    "    return None\n",
    "\n",
    "def read_labels_from_excel(excel_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    labels_dict = pd.Series(df.Label.values, index=df['Image Name']).to_dict()\n",
    "    return labels_dict\n",
    "\n",
    "# Function to compare predicted and actual labels for similarity\n",
    "def compare_label_similarity(predicted_label, actual_label):\n",
    "    return fuzz.ratio(predicted_label.lower(), actual_label.lower())\n",
    "\n",
    "# Main processing function\n",
    "def process_images_and_compare_labels(folder_path, excel_path):\n",
    "    actual_labels_dict = read_labels_from_excel(excel_path)\n",
    "    similarities = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            predicted_label = predict_image_label(img_path)\n",
    "            actual_label = actual_labels_dict.get(filename)\n",
    "\n",
    "            if predicted_label and actual_label:\n",
    "                similarity_score = compare_label_similarity(predicted_label, actual_label)\n",
    "                similarities.append(similarity_score)\n",
    "                print(f\"{filename}: Predicted Label - {predicted_label}, Actual Label - {actual_label}, Similarity - {similarity_score}%\")\n",
    "\n",
    "    if similarities:\n",
    "        average_similarity = sum(similarities) / len(similarities)\n",
    "        print(f\"Average Label Similarity: {average_similarity}%\")\n",
    "    else:\n",
    "        print(\"No images processed.\")\n",
    "\n",
    "# Update with your actual folder and Excel paths\n",
    "folder_path = 'Example Data-20240208T214429Z-001/Example Data/exported'\n",
    "excel_path = 'Example Data-20240208T214429Z-001/Example Data/Imagelabels.xlsx'\n",
    "\n",
    "process_images_and_compare_labels(folder_path, excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "(1) @GreyCupFestival - 109th Grey Cup.jpeg: Predicted - stage, Actual - Glasses, Similarity - 33%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "(10) 17887803224903630.jpeg: Predicted - seashore, Actual - Woman, Similarity - 15%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "(12) 17985809330117499.jpeg: Predicted - moped, Actual - Night, Similarity - 0%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "(13) 18013990822817757.jpeg: Predicted - jersey, Actual - Paper, Similarity - 36%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "(14) 17993584322154200.jpeg: Predicted - cliff, Actual - Lion, Similarity - 44%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "(15) 18346855723078911.jpeg: Predicted - street_sign, Actual - Poster, Similarity - 35%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "(16) 18379894042056715.jpeg: Predicted - cowboy_hat, Actual - Head, Similarity - 29%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "(17) 17876557646942156.jpeg: Predicted - football_helmet, Actual - Hand, Similarity - 11%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "(18) 17956303754673865.jpeg: Predicted - football_helmet, Actual - People, Similarity - 29%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "(19) 18027245935589987.jpeg: Predicted - book_jacket, Actual - People, Similarity - 24%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "(2) 18040499875507660.jpeg: Predicted - ski, Actual - Helmet, Similarity - 0%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "(2) @VanArchives - Vancouver Archives.jpeg: Predicted - mortarboard, Actual - Audience, Similarity - 21%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "(2) ball-park-brand-mflmvznfdq8-unsplash.jpeg: Predicted - conch, Actual - Shoe, Similarity - 22%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "(3) 18005670805793076.jpeg: Predicted - moped, Actual - Car Wheel, Similarity - 14%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "(3) @NewEraCanada - New Era Canada.jpeg: Predicted - jersey, Actual - QR Code, Similarity - 15%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "(4) 17978197394393810.jpeg: Predicted - motor_scooter, Actual - Shoe, Similarity - 24%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "(4) @betregal - BetRegal.jpeg: Predicted - web_site, Actual - Text, Similarity - 33%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "(5) @Stats_Junkie - Chris ðŸ‡¨ðŸ‡¦ðŸˆ Stats Junkie.png: Predicted - menu, Actual - Measurements, Similarity - 38%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "(6) 18030065674525036.jpeg: Predicted - web_site, Actual - Poster, Similarity - 43%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "(6) @Tara_Marie29 - Tara-Marie Hall.jpeg: Predicted - basketball, Actual - T-Shirt, Similarity - 24%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "(7) @Cdn_Turkey - Canadian Turkey.jpeg: Predicted - soup_bowl, Actual - Pancake, Similarity - 12%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "(8) 17963380910618789.jpeg: Predicted - rapeseed, Actual - Bird, Similarity - 33%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "(8) @fisherwavy - Fisher Wavy.jpeg: Predicted - web_site, Actual - Appliance, Similarity - 12%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "IMG_0866 Large.jpeg: Predicted - stage, Actual - Outdoors, Similarity - 15%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "IMG_2318 Large.jpeg: Predicted - restaurant, Actual - Audience, Similarity - 33%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "IMG_2334 Large.jpeg: Predicted - dome, Actual - Urban, Similarity - 0%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "IMG_2345 Large.jpeg: Predicted - stage, Actual - Head, Similarity - 22%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "IMG_3155.JPG: Predicted - park_bench, Actual - Shoe, Similarity - 14%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "IMG_3386 Large.jpeg: Predicted - freight_car, Actual - Airliner, Similarity - 32%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "IMG_3614 Large.jpeg: Predicted - patio, Actual - Rural, Similarity - 20%\n",
      "\n",
      "Average Label Similarity: 22.766666666666666%\n",
      "The system does not meet the required similarity threshold of 80%. System evaluation failed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main processing and evaluation function\n",
    "def evaluate_image_labeling_system(folder_path, excel_path, similarity_threshold=80):\n",
    "    actual_labels_dict = read_labels_from_excel(excel_path)\n",
    "    similarities = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            predicted_label = predict_image_label(img_path)\n",
    "            actual_label = actual_labels_dict.get(filename)\n",
    "\n",
    "            if predicted_label and actual_label:\n",
    "                similarity_score = compare_label_similarity(predicted_label, actual_label)\n",
    "                similarities.append(similarity_score)\n",
    "                print(f\"{filename}: Predicted - {predicted_label}, Actual - {actual_label}, Similarity - {similarity_score}%\")\n",
    "\n",
    "    if similarities:\n",
    "        average_similarity = sum(similarities) / len(similarities)\n",
    "        print(f\"\\nAverage Label Similarity: {average_similarity}%\")\n",
    "        \n",
    "        if average_similarity >= similarity_threshold:\n",
    "            print(f\"The system meets the required similarity threshold of {similarity_threshold}%. System evaluation passed.\")\n",
    "        else:\n",
    "            print(f\"The system does not meet the required similarity threshold of {similarity_threshold}%. System evaluation failed.\")\n",
    "    else:\n",
    "        print(\"No images processed. Evaluation cannot be performed.\")\n",
    "\n",
    "\n",
    "evaluate_image_labeling_system(folder_path, excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune the pretrained model to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Preparing the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess labels\n",
    "excel_path = 'Example Data-20240208T214429Z-001/Example Data/Imagelabels.xlsx'\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# Assuming 'Image Name' column has the image filenames and 'Label' column has labels separated by some delimiter (e.g., ',')\n",
    "df['Label'] = df['Label'].apply(lambda x: x.split(','))  # Split labels into lists\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(df['Label'])\n",
    "\n",
    "# Assuming image filenames are unique and map one-to-one to labels\n",
    "labels_dict = dict(zip(df['Image Name'], labels))\n",
    "\n",
    "# Load and preprocess images\n",
    "folder_path = 'Example Data-20240208T214429Z-001/Example Data/exported'\n",
    "image_size = (299, 299)  # InceptionV3 default image size\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    img = image.load_img(path, target_size=image_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "#  images are stored directly in folder_path with filenames matching those in the Excel file\n",
    "images = [load_and_preprocess_image(os.path.join(folder_path, fname)) for fname in labels_dict.keys()]\n",
    "images = np.array(images)\n",
    "\n",
    "# Split data into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, list(labels_dict.values()), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Modifying and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.0000e+00 - loss: 0.7332 - val_accuracy: 0.0000e+00 - val_loss: 0.2945\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 0.3108 - val_accuracy: 0.0000e+00 - val_loss: 0.0959\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 0.1147 - val_accuracy: 0.0000e+00 - val_loss: 0.0390\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 0.0499 - val_accuracy: 0.0000e+00 - val_loss: 0.0354\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.0417 - loss: 0.0377 - val_accuracy: 0.0000e+00 - val_loss: 0.0415\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.1667 - loss: 0.0358 - val_accuracy: 0.1429 - val_loss: 0.0479\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.1667 - loss: 0.0336 - val_accuracy: 0.1429 - val_loss: 0.0532\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.1667 - loss: 0.0298 - val_accuracy: 0.1429 - val_loss: 0.0588\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.1667 - loss: 0.0259 - val_accuracy: 0.1429 - val_loss: 0.0630\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3333 - loss: 0.0206 - val_accuracy: 0.0000e+00 - val_loss: 0.0684\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6250 - loss: 0.0170 - val_accuracy: 0.0000e+00 - val_loss: 0.0746\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6667 - loss: 0.0162 - val_accuracy: 0.0000e+00 - val_loss: 0.0777\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7917 - loss: 0.0140 - val_accuracy: 0.1429 - val_loss: 0.0798\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8333 - loss: 0.0113 - val_accuracy: 0.1429 - val_loss: 0.0824\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.9167 - loss: 0.0092 - val_accuracy: 0.1429 - val_loss: 0.0855\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.9583 - loss: 0.0079 - val_accuracy: 0.0000e+00 - val_loss: 0.0891\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.9583 - loss: 0.0074 - val_accuracy: 0.0000e+00 - val_loss: 0.0922\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.1429 - val_loss: 0.0945\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.1429 - val_loss: 0.0964\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.1429 - val_loss: 0.0982\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.0000e+00 - val_loss: 0.0999\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.0000e+00 - val_loss: 0.1015\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.1429 - val_loss: 0.1028\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.1429 - val_loss: 0.1040\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.1429 - val_loss: 0.1052\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.1429 - val_loss: 0.1064\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.1429 - val_loss: 0.1076\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.1429 - val_loss: 0.1089\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.1429 - val_loss: 0.1102\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.1429 - val_loss: 0.1116\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.1429 - val_loss: 0.1130\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.1429 - val_loss: 0.1145\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 9.3463e-04 - val_accuracy: 0.1429 - val_loss: 0.1159\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 8.7626e-04 - val_accuracy: 0.0000e+00 - val_loss: 0.1172\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 8.3053e-04 - val_accuracy: 0.0000e+00 - val_loss: 0.1184\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 7.8712e-04 - val_accuracy: 0.0000e+00 - val_loss: 0.1195\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 7.4151e-04 - val_accuracy: 0.0000e+00 - val_loss: 0.1205\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 6.9330e-04 - val_accuracy: 0.0000e+00 - val_loss: 0.1212\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 6.4428e-04 - val_accuracy: 0.0000e+00 - val_loss: 0.1218\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.9657e-04 - val_accuracy: 0.0000e+00 - val_loss: 0.1223\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.5169e-04 - val_accuracy: 0.1429 - val_loss: 0.1226\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.1026e-04 - val_accuracy: 0.1429 - val_loss: 0.1228\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.7226e-04 - val_accuracy: 0.1429 - val_loss: 0.1228\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.3758e-04 - val_accuracy: 0.1429 - val_loss: 0.1229\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.0643e-04 - val_accuracy: 0.1429 - val_loss: 0.1228\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.7943e-04 - val_accuracy: 0.1429 - val_loss: 0.1228\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.5708e-04 - val_accuracy: 0.1429 - val_loss: 0.1227\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.3946e-04 - val_accuracy: 0.1429 - val_loss: 0.1226\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.2591e-04 - val_accuracy: 0.2857 - val_loss: 0.1226\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.1534e-04 - val_accuracy: 0.2857 - val_loss: 0.1226\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.0649e-04 - val_accuracy: 0.2857 - val_loss: 0.1226\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.9828e-04 - val_accuracy: 0.1429 - val_loss: 0.1227\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.8992e-04 - val_accuracy: 0.1429 - val_loss: 0.1227\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.8107e-04 - val_accuracy: 0.1429 - val_loss: 0.1228\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.7175e-04 - val_accuracy: 0.1429 - val_loss: 0.1229\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.6227e-04 - val_accuracy: 0.1429 - val_loss: 0.1231\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.5300e-04 - val_accuracy: 0.1429 - val_loss: 0.1233\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.4429e-04 - val_accuracy: 0.1429 - val_loss: 0.1234\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.3632e-04 - val_accuracy: 0.1429 - val_loss: 0.1236\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.2917e-04 - val_accuracy: 0.1429 - val_loss: 0.1238\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.2276e-04 - val_accuracy: 0.1429 - val_loss: 0.1240\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.1697e-04 - val_accuracy: 0.1429 - val_loss: 0.1241\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.1164e-04 - val_accuracy: 0.1429 - val_loss: 0.1243\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.0665e-04 - val_accuracy: 0.1429 - val_loss: 0.1245\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.0191e-04 - val_accuracy: 0.1429 - val_loss: 0.1246\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.9737e-04 - val_accuracy: 0.1429 - val_loss: 0.1248\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.9303e-04 - val_accuracy: 0.1429 - val_loss: 0.1249\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.8891e-04 - val_accuracy: 0.1429 - val_loss: 0.1250\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.8503e-04 - val_accuracy: 0.1429 - val_loss: 0.1251\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.8138e-04 - val_accuracy: 0.1429 - val_loss: 0.1252\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.7796e-04 - val_accuracy: 0.1429 - val_loss: 0.1253\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.7476e-04 - val_accuracy: 0.1429 - val_loss: 0.1253\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.7173e-04 - val_accuracy: 0.1429 - val_loss: 0.1254\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.6884e-04 - val_accuracy: 0.1429 - val_loss: 0.1255\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.6605e-04 - val_accuracy: 0.1429 - val_loss: 0.1255\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.6333e-04 - val_accuracy: 0.1429 - val_loss: 0.1256\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.6068e-04 - val_accuracy: 0.1429 - val_loss: 0.1256\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.5807e-04 - val_accuracy: 0.1429 - val_loss: 0.1257\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.5552e-04 - val_accuracy: 0.1429 - val_loss: 0.1257\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.5304e-04 - val_accuracy: 0.1429 - val_loss: 0.1257\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.5064e-04 - val_accuracy: 0.1429 - val_loss: 0.1258\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.4833e-04 - val_accuracy: 0.1429 - val_loss: 0.1258\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.4612e-04 - val_accuracy: 0.1429 - val_loss: 0.1258\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.4401e-04 - val_accuracy: 0.1429 - val_loss: 0.1258\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.4200e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.4007e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.3821e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.3643e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.3470e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.3301e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.3136e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.2975e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.2816e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.2660e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.2507e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.2357e-04 - val_accuracy: 0.1429 - val_loss: 0.1259\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.2210e-04 - val_accuracy: 0.1429 - val_loss: 0.1260\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.2067e-04 - val_accuracy: 0.1429 - val_loss: 0.1260\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.1926e-04 - val_accuracy: 0.1429 - val_loss: 0.1260\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.1789e-04 - val_accuracy: 0.1429 - val_loss: 0.1260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x257a9f26300>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load InceptionV3 pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add new layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(mlb.classes_), activation='softmax')(x)  # 'sigmoid' for multi-label classification\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the new data\n",
    "model.fit(X_train, np.array(y_train), validation_data=(X_val, np.array(y_val)), epochs=100, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
