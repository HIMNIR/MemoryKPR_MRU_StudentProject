{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 170ms/step - accuracy: 0.0188 - loss: 5.2385 - val_accuracy: 0.0402 - val_loss: 5.1084\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - accuracy: 0.0372 - loss: 5.0334 - val_accuracy: 0.0402 - val_loss: 5.1391\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 0.0696 - loss: 4.8993 - val_accuracy: 0.0352 - val_loss: 5.2069\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.0732 - loss: 4.8312 - val_accuracy: 0.0251 - val_loss: 5.4152\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 0.0919 - loss: 4.6113 - val_accuracy: 0.0201 - val_loss: 5.1267\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.0760 - loss: 4.5876 - val_accuracy: 0.0151 - val_loss: 5.5724\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 0.0702 - loss: 4.2957 - val_accuracy: 0.0101 - val_loss: 5.7509\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.1202 - loss: 4.0656 - val_accuracy: 0.0050 - val_loss: 5.6959\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - accuracy: 0.1019 - loss: 3.9305 - val_accuracy: 0.0000e+00 - val_loss: 6.1819\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.0975 - loss: 3.8633 - val_accuracy: 0.0000e+00 - val_loss: 5.8309\n",
      "7/7 - 0s - 38ms/step - accuracy: 0.0000e+00 - loss: 5.8309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "folder_path = 'Example Data-20240208T214429Z-001/Example Data/exported'\n",
    "labels_path = 'Example Data-20240208T214429Z-001/Example Data/ImageLabels.xlsx'\n",
    "image_name_column = 'Image Name'  \n",
    "\n",
    "# Load the labels\n",
    "labels_df = pd.read_excel(labels_path)\n",
    "\n",
    "# Prepare the data arrays\n",
    "images = []\n",
    "valid_labels = []\n",
    "\n",
    "# Load and preprocess the images\n",
    "for index, row in labels_df.iterrows():\n",
    "    image_name = row[image_name_column]\n",
    "    image_path = os.path.join(folder_path, image_name)\n",
    "    if os.path.exists(image_path):\n",
    "        # Open the image\n",
    "        with Image.open(image_path) as img:\n",
    "            # Preprocess the image as required, e.g., resize\n",
    "            img = img.resize((128, 128))  # Example resizing to 128x128\n",
    "            img = img.convert('RGB')  # Convert to RGB if needed\n",
    "\n",
    "            # Convert the image to a numpy array and normalize pixel values\n",
    "            img_array = np.array(img) / 255.0\n",
    "\n",
    "            # Append the image and label to the lists\n",
    "            images.append(img_array)\n",
    "            valid_labels.append(row['Label'])  # Replace 'Label' with your actual label column name\n",
    "    else:\n",
    "        print(f\"Image {image_name} not found.\")\n",
    "\n",
    "# Convert images to a numpy array\n",
    "images_array = np.array(images)\n",
    "\n",
    "# Encode the valid labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(valid_labels)\n",
    "one_hot_labels = to_categorical(encoded_labels, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "# Now we can be sure the images and labels match\n",
    "assert len(images_array) == len(one_hot_labels), \"The number of images does not match the number of labels.\"\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_array, one_hot_labels, test_size=0.50, random_state=42)\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The .h5 file format is commonly used to store trained models in Keras. It contains the architecture of the model, the weights of the model, the training configuration (loss, optimizer), and the state of the optimizer, allowing to resume training exactly where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <KerasVariable shape=(3, 3, 3, 32), dtype=float32, path=sequential_3/conv2d_9/kernel>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_model_best.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Continue training the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Define how many additional epochs to train\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluate the model again\u001b[39;00m\n\u001b[0;32m     24\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:215\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[1;34m(self, variables)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    216\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown variable: <KerasVariable shape=(3, 3, 3, 32), dtype=float32, path=sequential_3/conv2d_9/kernel>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "additional_epochs = 10\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# Define early stopping and model checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_checkpoint = ModelCheckpoint('my_model_best.keras', save_best_only=True)\n",
    "\n",
    "# Continue training the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=additional_epochs,  # Define how many additional epochs to train\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the model again\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# To calculate the F1 score,  to make predictions and compare  to the true labels\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate F1 score\n",
    "report = classification_report(y_true, y_pred_classes, output_dict=True)\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
