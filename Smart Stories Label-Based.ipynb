{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\ericn\\AppData\\Local\\Temp\\ipykernel_21624\\1619158158.py:4: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  LABELS_DIR_PATH = 'Example Data-20240208T214429Z-001\\Example Data\\ImageLabels.xlsx'\n",
      "C:\\Users\\ericn\\AppData\\Local\\Temp\\ipykernel_21624\\1619158158.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Labels'] = filtered_df['Instance Count'].astype(str) + ' ' + filtered_df['Label']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Name</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1) @GreyCupFestival - 109th Grey Cup.jpeg</td>\n",
       "      <td>2 Adult, 1 Female, 7 Person, 1 Woman, 1 Male, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(10) 17887803224903630.jpeg</td>\n",
       "      <td>1 Horse, 1 Person, 1 Adult, 1 Female, 1 Woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(12) 17985809330117499.jpeg</td>\n",
       "      <td>1 Person, 1 Helmet, 1 Motorcycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(13) 18013990822817757.jpeg</td>\n",
       "      <td>3 Passport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(14) 17993584322154200.jpeg</td>\n",
       "      <td>2 Person, 6 Bird, 1 Glove</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Image Name  \\\n",
       "0  (1) @GreyCupFestival - 109th Grey Cup.jpeg   \n",
       "1                 (10) 17887803224903630.jpeg   \n",
       "2                 (12) 17985809330117499.jpeg   \n",
       "3                 (13) 18013990822817757.jpeg   \n",
       "4                 (14) 17993584322154200.jpeg   \n",
       "\n",
       "                                              Labels  \n",
       "0  2 Adult, 1 Female, 7 Person, 1 Woman, 1 Male, ...  \n",
       "1      1 Horse, 1 Person, 1 Adult, 1 Female, 1 Woman  \n",
       "2                   1 Person, 1 Helmet, 1 Motorcycle  \n",
       "3                                         3 Passport  \n",
       "4                          2 Person, 6 Bird, 1 Glove  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the ImageLabels.xlsx file into a pandas dataframe.\n",
    "LABELS_DIR_PATH = 'Example Data-20240208T214429Z-001\\Example Data\\ImageLabels.xlsx'\n",
    "df = pd.read_excel(LABELS_DIR_PATH, usecols=['Image Name', 'Confidence', 'Instance Count', 'Label'])\n",
    "\n",
    "# Step 2: Process the dataframe.\n",
    "# Remove labels with a confidence value < 80 and instance count of 0.\n",
    "filtered_df = df[(df['Confidence'] >= 80) & (df['Instance Count'] > 0)]\n",
    "\n",
    "# Append the instance counts to the label.\n",
    "filtered_df['Labels'] = filtered_df['Instance Count'].astype(str) + ' ' + filtered_df['Label']\n",
    "\n",
    "# Aggregate labels for each image into a single row.\n",
    "aggregated_df = filtered_df.groupby('Image Name')['Labels'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "\n",
    "aggregated_df.shape # Checking the size of the labels file. \n",
    "aggregated_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(1) @GreyCupFestival - 109th Grey Cup.jpeg': 'A picture containing: 2 Adult, 1 Female, 7 Person, 1 Woman, 1 Male, 1 Man, 1 Helmet, 1 Coat, 1 Shoe', '(10) 17887803224903630.jpeg': 'A picture containing: 1 Horse, 1 Person, 1 Adult, 1 Female, 1 Woman', '(12) 17985809330117499.jpeg': 'A picture containing: 1 Person, 1 Helmet, 1 Motorcycle', '(13) 18013990822817757.jpeg': 'A picture containing: 3 Passport', '(14) 17993584322154200.jpeg': 'A picture containing: 2 Person, 6 Bird, 1 Glove', '(16) 18379894042056715.jpeg': 'A picture containing: 2 Person, 2 Shaker, 1 Adult, 1 Female, 1 Woman, 1 Baseball Cap', '(17) 17876557646942156.jpeg': 'A picture containing: 3 Person, 2 Adult, 2 Male, 2 Man, 1 Glove, 1 Helmet, 4 Shoe', '(18) 17956303754673865.jpeg': 'A picture containing: 1 Helmet, 1 Person, 1 Adult, 1 Male, 1 Man, 2 Glove', '(19) 18027245935589987.jpeg': 'A picture containing: 1 Helmet, 1 Adult, 1 Male, 1 Man, 1 Person, 2 Glove, 2 Shoe', '(2) 18040499875507660.jpeg': 'A picture containing: 2 Person, 1 Boy, 1 Child, 1 Male, 1 Glove', '(2) @VanArchives - Vancouver Archives.jpeg': 'A picture containing: 14 Person, 3 Adult, 2 Male, 2 Man, 1 Female, 1 Woman', '(2) ball-park-brand-mflmvznfdq8-unsplash.jpeg': 'A picture containing: 4 Adult, 3 Female, 5 Person, 3 Woman, 1 Bonfire, 1 Male, 1 Man', '(3) 18005670805793076.jpeg': 'A picture containing: 1 Adult, 1 Male, 1 Man, 2 Person, 1 Motorcycle, 4 Car, 3 Wheel, 1 Glove', '(3) @NewEraCanada - New Era Canada.jpeg': 'A picture containing: 2 Hat', '(4) 17978197394393810.jpeg': 'A picture containing: 4 Adult, 4 Male, 4 Man, 10 Person, 2 Motorcycle, 2 Wheel, 1 E-scooter', '(4) @betregal - BetRegal.jpeg': 'A picture containing: 1 Scoreboard', '(6) @Tara_Marie29 - Tara-Marie Hall.jpeg': 'A picture containing: 1 Rugby Ball, 1 Person, 1 Boy, 1 Child, 1 Male, 1 American Football (Ball)', '(7) @Cdn_Turkey - Canadian Turkey.jpeg': 'A picture containing: 1 Pizza', '(8) 17963380910618789.jpeg': 'A picture containing: 1 Shoe', '(8) @fisherwavy - Fisher Wavy.jpeg': 'A picture containing: 1 Person', 'IMG_0866 Large.jpeg': 'A picture containing: 11 Person', 'IMG_2318 Large.jpeg': 'A picture containing: 3 Adult, 1 Female, 13 Person, 1 Woman, 1 Building, 2 Male, 2 Man, 2 Plant', 'IMG_2334 Large.jpeg': 'A picture containing: 3 Building', 'IMG_2345 Large.jpeg': 'A picture containing: 3 Person, 2 Adult, 2 Male, 2 Man, 1 Plant, 3 Shoe, 1 Laptop, 1 Chair', 'IMG_3155.JPG': 'A picture containing: 3 Coat, 2 Adult, 2 Male, 2 Man, 2 Person, 2 Glove, 1 Jacket, 2 Car', 'IMG_3386 Large.jpeg': 'A picture containing: 1 Airplane', 'IMG_3614 Large.jpeg': 'A picture containing: 12 Person, 1 Handbag, 1 Traffic Light'}\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy array\n",
    "labels_array = aggregated_df.to_numpy()\n",
    "\n",
    "# Seems to make help the model make better predictions.\n",
    "LABELS_CONTEXT_STRING = 'A picture containing: '\n",
    "# Convert to dictionary\n",
    "labels_dict = dict(zip(aggregated_df['Image Name'], LABELS_CONTEXT_STRING + aggregated_df['Labels']))\n",
    "\n",
    "print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user input and smart story description for a given image\n",
    "user_text_input = \"A picture of a person on a motorcycle.\"\n",
    "smart_story_description = \"Adam enjoying the beautiful day on his motorcycle.\"\n",
    "\n",
    "# Assuming you're working with a single image and its labels\n",
    "image_name = \"(12) 17985809330117499.jpeg\" # motorcycle image, just for testing\n",
    "image_labels = labels_dict.get(image_name, \"\")\n",
    "\n",
    "# Preprocess and tokenize\n",
    "texts = [user_text_input, smart_story_description, image_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states  # Get hidden states\n",
    "        # Use the embeddings of the last layer's `[CLS]` token for sentence representation\n",
    "        sentence_embedding = hidden_states[-1][:, 0, :].squeeze().detach().numpy()\n",
    "        embeddings.append(sentence_embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between User Text Input and Labels: 0.8478407859802246\n",
      "Similarity between Smart Story Description and Labels: 0.726824164390564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you've obtained embeddings\n",
    "embeddings = get_sentence_embeddings(texts, model, tokenizer)\n",
    "\n",
    "# Compute cosine similarity\n",
    "# Note: embeddings[0] is user_text_input, embeddings[1] is smart_story_description, embeddings[2] is image_labels\n",
    "similarity_input_label = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]\n",
    "similarity_story_label = cosine_similarity([embeddings[1]], [embeddings[2]])[0][0]\n",
    "\n",
    "print(f\"Similarity between User Text Input and Labels: {similarity_input_label}\")\n",
    "print(f\"Similarity between Smart Story Description and Labels: {similarity_story_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
